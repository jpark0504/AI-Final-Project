{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up & Installation"
      ],
      "metadata": {
        "id": "On_6cU-eRaVJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90YRYqbwRXT7",
        "outputId": "64b9981b-a335-4558-b68d-e2057c37ab13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (0.12.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install librosa torchaudio soundfile omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VHZqy2NRh5P",
        "outputId": "a07ed67a-93fc-4ec4-a4a5-0186ad043b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "2.5.1+cu121\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x79aae83d9a50>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting Attributes"
      ],
      "metadata": {
        "id": "ad3h8JVqRmqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in dataset from Kaggle\n",
        "import os\n",
        "import shutil\n",
        "import kagglehub\n",
        "\n",
        "content_folder = \"/content/audio_emotions_dataset\"\n",
        "os.makedirs(content_folder, exist_ok=True)\n",
        "path = kagglehub.dataset_download(\"uldisvalainis/audio-emotions\")\n",
        "shutil.move(path, content_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "hxxazjT8RpHY",
        "outputId": "0ccc0162-2713-4bf3-ce51-e792eaefb483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/uldisvalainis/audio-emotions?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.12G/1.12G [00:15<00:00, 80.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/audio_emotions_dataset/1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/audio_emotions_dataset/1\"\n",
        "output_json = \"audio_features.json\"\n",
        "\n",
        "# Extract features and transcriptions\n",
        "def extract_features_and_transcriptions(audio_data):\n",
        "    features_and_text = []\n",
        "    skipped_files = []\n",
        "\n",
        "    # Initialize the ASR model\n",
        "    model, decoder, utils = torch.hub.load(\n",
        "        repo_or_dir='snakers4/silero-models',\n",
        "        model='silero_stt',\n",
        "        language='en',\n",
        "        device=torch.device('cpu')\n",
        "    )\n",
        "    (read_batch, split_into_batches, read_audio, prepare_model_input) = utils\n",
        "\n",
        "    for file_path, label in tqdm(audio_data, desc=\"Processing audio\"):\n",
        "        try:\n",
        "            # Extract features\n",
        "            waveform, sr = librosa.load(file_path, sr=16000)\n",
        "            mfccs = librosa.feature.mfcc(y=waveform, sr=sr, n_mfcc=13).mean(axis=1).tolist()\n",
        "\n",
        "            # Transcribe audio\n",
        "            wav = read_audio(file_path)\n",
        "            input_data = prepare_model_input([wav], device=torch.device('cpu'))\n",
        "            output = model(input_data)\n",
        "            transcription = decoder(output[0].cpu())\n",
        "\n",
        "            # Save results\n",
        "            features_and_text.append({\n",
        "                \"file_name\": file_path,\n",
        "                \"label\": label,\n",
        "                \"features\": mfccs,\n",
        "                \"transcription\": transcription\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            skipped_files.append(file_path)\n",
        "\n",
        "    return features_and_text, skipped_files\n",
        "\n",
        "def get_audio_data_limited(dataset_path, max_files_per_label):\n",
        "    \"\"\"\n",
        "    Scans dataset directories for audio files and assigns labels based on directory structure.\n",
        "    Limits the number of files processed per label.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    label_file_count = {}\n",
        "\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        label = os.path.basename(root)\n",
        "        if label not in label_file_count:\n",
        "            label_file_count[label] = 0\n",
        "\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                if label_file_count[label] < max_files_per_label:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    data.append((file_path, label))\n",
        "                    label_file_count[label] += 1\n",
        "                else:\n",
        "                    break\n",
        "    return data"
      ],
      "metadata": {
        "id": "PKq7ESPRRrak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Extract\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import json\n",
        "\n",
        "MAX_FILES_PER_LABEL = 100\n",
        "\n",
        "dataset_path = \"/content/audio_emotions_dataset/1\"\n",
        "output_json = \"limited_audio_data.json\"\n",
        "\n",
        "audio_data = get_audio_data_limited(dataset_path, max_files_per_label=MAX_FILES_PER_LABEL)\n",
        "\n",
        "features_and_text, skipped_files = extract_features_and_transcriptions(audio_data)\n",
        "\n",
        "output_data = {\n",
        "    \"features_and_text\": features_and_text,\n",
        "    \"skipped_files\": skipped_files\n",
        "}\n",
        "\n",
        "with open(output_json, \"w\") as f:\n",
        "    json.dump(output_data, f, indent=4)\n",
        "\n",
        "print(f\"Data saved to {output_json}.\")\n",
        "print(f\"Processed {len(features_and_text)} files, skipped {len(skipped_files)}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsVQtJuhR0Ae",
        "outputId": "08659229-a867-45c3-f23c-4345675fa17c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-models_master\n",
            "Processing audio: 100%|██████████| 700/700 [35:09<00:00,  3.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to limited_audio_data.json.\n",
            "Processed 700 files, skipped 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Model"
      ],
      "metadata": {
        "id": "J-6NmC8ZMS1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "json_file_path = 'limited_audio_data.json'\n",
        "\n",
        "texts = []\n",
        "numerical_data = []\n",
        "labels = []\n",
        "\n",
        "with open(json_file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for entry in data['features_and_text']:\n",
        "    texts.append(entry['transcription'])\n",
        "    numerical_data.append(entry['features'])\n",
        "    labels.append(entry[\"label\"])\n",
        "\n",
        "numerical_data = np.array(numerical_data)\n",
        "\n",
        "print(\"Texts:\", texts[:5])\n",
        "print(\"Numerical data:\", numerical_data[:5])\n",
        "print(\"Label: \", labels[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca9IkRPLsFvf",
        "outputId": "34fb71cb-40ba-4e61-9d40-9fa18ba1c8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texts: ['say the word gin', 'dogs are sitting by the door', 'say the word lot', 'say the word make', \"say the word' choice\"]\n",
            "Numerical data: [[-3.85439880e+02  5.75812721e+01  4.66590071e+00  8.06424999e+00\n",
            "  -6.90113544e-01 -1.65634060e+01 -7.00231647e+00 -9.07964897e+00\n",
            "  -1.29140091e+01 -1.13545475e+01 -1.73926449e+01  3.85375679e-01\n",
            "  -5.52141809e+00]\n",
            " [-5.13680298e+02  4.90157127e+01 -1.66620083e+01  9.16706181e+00\n",
            "  -7.41104031e+00 -8.39771271e+00 -1.50733442e+01 -2.14511418e+00\n",
            "  -1.14692354e+01  2.80825973e+00 -3.28063893e+00 -5.07822418e+00\n",
            "  -2.15373492e+00]\n",
            " [-3.21289337e+02  4.57948074e+01 -6.40001822e+00 -2.27347183e+01\n",
            "  -1.69492188e+01 -1.73506284e+00 -1.42188101e+01 -1.00522261e+01\n",
            "  -4.60576916e+00  1.38825047e+00 -6.79480457e+00  3.85209370e+00\n",
            "  -4.00710440e+00]\n",
            " [-3.91458252e+02  8.19596710e+01  5.75373697e+00  3.49292922e+00\n",
            "   2.79894185e+00 -1.56160812e+01 -4.93057728e+00 -1.49114742e+01\n",
            "  -9.84164715e+00 -1.44694710e+01 -1.09908476e+01 -5.05252063e-01\n",
            "  -5.12331104e+00]\n",
            " [-3.47835266e+02  9.73615742e+00  2.62046909e+00 -2.95279288e+00\n",
            "  -1.17402182e+01 -9.22664928e+00 -1.80139103e+01 -4.19263983e+00\n",
            "  -2.91903758e+00 -2.51349974e+00 -2.39491677e+00  3.15194559e+00\n",
            "   5.73866987e+00]]\n",
            "Label:  ['Suprised', 'Suprised', 'Suprised', 'Suprised', 'Suprised']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Load the data\n",
        "def load_data(json_file_path):\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    texts = []\n",
        "    numerical_data = []\n",
        "    labels = []\n",
        "\n",
        "    for entry in data['features_and_text']:\n",
        "        texts.append(entry['transcription'])\n",
        "        numerical_data.append(entry['features'])\n",
        "        labels.append(entry[\"label\"])\n",
        "\n",
        "    return texts, np.array(numerical_data), labels\n",
        "\n",
        "# Prepare the data\n",
        "def prepare_data(texts, numerical_data, labels):\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(labels)\n",
        "\n",
        "    X_text_train, X_text_test = [], []\n",
        "    X_num_train, X_num_test = [], []\n",
        "    y_train, y_test = [], []\n",
        "\n",
        "    unique_labels = np.unique(y)\n",
        "\n",
        "    for label in unique_labels:\n",
        "        label_indices = np.where(y == label)[0]\n",
        "\n",
        "        train_indices = label_indices[:80]\n",
        "        test_indices = label_indices[80:]\n",
        "\n",
        "        X_text_train.extend(np.array(texts)[train_indices])\n",
        "        X_num_train.extend(numerical_data[train_indices])\n",
        "        y_train.extend(y[train_indices])\n",
        "\n",
        "        X_text_test.extend(np.array(texts)[test_indices])\n",
        "        X_num_test.extend(numerical_data[test_indices])\n",
        "        y_test.extend(y[test_indices])\n",
        "\n",
        "    return (np.array(X_text_train), np.array(X_text_test),\n",
        "            np.array(X_num_train), np.array(X_num_test),\n",
        "            np.array(y_train), np.array(y_test), le)\n",
        "\n",
        "# Create a preprocessing pipeline\n",
        "def create_preprocessing_pipeline():\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    text_transformer = Pipeline(steps=[\n",
        "        ('tfidf', TfidfVectorizer(stop_words='english', max_features=100))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, [0]),\n",
        "            ('text', text_transformer, [1])\n",
        "        ])\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "# Combine text and numerical features\n",
        "def combine_features(X_num, X_text):\n",
        "    print(\"Initializing TF-IDF vectorizer...\")\n",
        "    tfidf = TfidfVectorizer(stop_words='english', max_features=100)\n",
        "    text_features = tfidf.fit_transform(X_text).toarray()\n",
        "\n",
        "    print(\"Combining numerical and text features...\")\n",
        "    combined_features = np.column_stack((X_num, text_features))\n",
        "\n",
        "    return combined_features\n",
        "\n",
        "# Train and evaluate models\n",
        "def train_and_evaluate_models(X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test, le):\n",
        "    X_train_combined = combine_features(X_num_train, X_text_train)\n",
        "    X_test_combined = combine_features(X_num_test, X_text_test)\n",
        "\n",
        "    models = {\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=1000, max_depth=30, random_state=42),\n",
        "        'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50, 25, 20), max_iter=250, learning_rate_init=0.0005, random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nTraining {name}...\")\n",
        "\n",
        "        model.fit(X_train_combined, y_train)\n",
        "        y_pred = model.predict(X_test_combined)\n",
        "\n",
        "        results[name] = {\n",
        "            'classification_report': classification_report(y_test, y_pred, target_names=le.classes_),\n",
        "            'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
        "        }\n",
        "\n",
        "        print(f\"{name} Results:\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(results[name]['classification_report'])\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(results[name]['confusion_matrix'])\n",
        "\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "    json_file_path = 'limited_audio_data.json'\n",
        "    texts, numerical_data, labels = load_data(json_file_path)\n",
        "    X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test, le = prepare_data(texts, numerical_data, labels)\n",
        "    results = train_and_evaluate_models( X_text_train, X_text_test, X_num_train, X_num_test, y_train, y_test, le)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeP_Z4kAwHtU",
        "outputId": "b0835582-adbb-4df4-e22a-790f28049035"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing TF-IDF vectorizer...\n",
            "Combining numerical and text features...\n",
            "Initializing TF-IDF vectorizer...\n",
            "Combining numerical and text features...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       0.47      0.95      0.63        20\n",
            "   Disgusted       0.25      0.15      0.19        20\n",
            "     Fearful       0.57      0.20      0.30        20\n",
            "       Happy       0.15      0.15      0.15        20\n",
            "     Neutral       0.35      0.40      0.37        20\n",
            "         Sad       0.58      0.55      0.56        20\n",
            "    Suprised       0.53      0.50      0.51        20\n",
            "\n",
            "    accuracy                           0.41       140\n",
            "   macro avg       0.41      0.41      0.39       140\n",
            "weighted avg       0.41      0.41      0.39       140\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[19  0  0  1  0  0  0]\n",
            " [ 2  3  0  5  7  2  1]\n",
            " [ 4  1  4  6  2  3  0]\n",
            " [ 6  4  1  3  1  0  5]\n",
            " [ 1  1  0  5  8  3  2]\n",
            " [ 1  2  0  0  5 11  1]\n",
            " [ 7  1  2  0  0  0 10]]\n",
            "\n",
            "Training Neural Network...\n",
            "Neural Network Results:\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       0.85      0.85      0.85        20\n",
            "   Disgusted       0.29      0.40      0.33        20\n",
            "     Fearful       0.56      0.25      0.34        20\n",
            "       Happy       0.20      0.20      0.20        20\n",
            "     Neutral       0.33      0.30      0.32        20\n",
            "         Sad       0.65      0.65      0.65        20\n",
            "    Suprised       0.60      0.75      0.67        20\n",
            "\n",
            "    accuracy                           0.49       140\n",
            "   macro avg       0.50      0.49      0.48       140\n",
            "weighted avg       0.50      0.49      0.48       140\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[17  1  0  2  0  0  0]\n",
            " [ 1  8  0  5  3  2  1]\n",
            " [ 1  4  5  6  2  2  0]\n",
            " [ 1  5  2  4  1  0  7]\n",
            " [ 0  8  1  1  6  3  1]\n",
            " [ 0  0  0  1  5 13  1]\n",
            " [ 0  2  1  1  1  0 15]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (250) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}